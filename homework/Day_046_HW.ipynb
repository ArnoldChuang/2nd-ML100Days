{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取鳶尾花資料集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Acuuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業\n",
    "目前已經學過許多的模型，相信大家對整體流程應該比較掌握了，這次作業請改用**手寫辨識資料集**，步驟流程都是一樣的，請試著自己撰寫程式碼來完成所有步驟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取料集\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立 GradientBoostingClassifier 模型\n",
    "## n_estimators: 也就是弱學習器的最大遞迴次數，或者說最大的弱學習器的個數。\n",
    "# 一般来說 n_estimators 太小，容易欠擬合，n_estimators太大，又容易過擬合，\n",
    "# 一般選擇一個適中的數值。預設是100。\n",
    "\n",
    "## earning_rate: 即每個弱學習器的權重缩減系数 ν，也稱作步長，\n",
    "# 加上了正規化項，我们的强學習器的遞迴公式為 fk(x)=fk−1(x)+νhk(x)。ν的取值範圍為 0<ν≤1。\n",
    "# 對於同樣的訓練集擬合效果，較小的 ν 意味著我们需要更多的弱學習器的遞迴次數。\n",
    "# 通常我们用步長和遞迴最大次數一起來决定演算法的擬合效果。所以這兩個参數 n_estimators和 learning_rate要一起調。\n",
    "# 一般來說，可以從一個小一點的 ν開始调，預設是1\n",
    "\n",
    "## subsample: 子採樣，取值為 (0,1]。\n",
    "# 注意這裡的子採樣和隨機森林不一樣，随機森林使用的是放回抽樣，而這裡是不放回抽樣。\n",
    "# 如果取值為 1，則全部樣本都使用，等於没有使用子採樣。如果取值小於1，則只有一部分樣本會去做 GBDT的決策樹擬合。\n",
    "# 選擇小於 1的比例可以減少方差，即防止過擬合，但是會增加樣本擬合的偏差，因此取值不能太低。\n",
    "# 推薦在[0.5, 0.8]之間，預設是1.0，即不使用子採樣\n",
    "\n",
    "## init: 如果不输入，則用訓練集樣本来做樣本集的初始化分類回歸预测。\n",
    "# 一般用在我們對数據有先驗知識，或者之前做过一些擬合的時候，如果沒有的話就不用管這個參數了\n",
    "\n",
    "## loss: GBDT演算法中的損失函數。分類模型和回歸模型的損失函数是不一樣的。\n",
    "# 對於分類模型，有對數似然損失函数 \"deviance\"和指數損失函数 \"exponential\"兩個選擇。\n",
    "# 預設是對數似然損失函数 \"deviance\"。\n",
    "# \"deviance\" 它對二元分法和多元分類各自都有比較好的優化。而指數損失函数等於带到了Adaboost演算法\n",
    "\n",
    "## alpha：這個參數只有 GradientBoostingRegressor有，\n",
    "# 當我們使用 Huber損失 \"huber\"和分位數損失 “quantile”時，需要指定分位數的值。預設是 0.9，如果躁點較多，可以適當降低這個分位數的值\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Acuuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
